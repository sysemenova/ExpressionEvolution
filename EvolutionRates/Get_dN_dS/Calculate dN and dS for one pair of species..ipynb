{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os.path\n",
    "import os\n",
    "#you will need to install library biopython\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_dna, generic_protein\n",
    "from Bio.Phylo.PAML import codeml\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse fasta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CDS for Human and Mouse from https://useast.ensembl.org/info/data/ftp/index.html\n",
    "# for bactarial species from https://bacteria.ensembl.org/info/website/ftp/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#species I will use \n",
    "list_sp=['H37Ra','Haemophilum'] # for first paper ['BW25113','Senterica'] , for second paper ['EcoliRel606','Senterica']  \n",
    "# a dictinary with full names of CDS files\n",
    "dist_sp=dist_sp={\\\n",
    "    'Senterica':'Salmonella_enterica.ASM78381v1.cds.all.fa',\\\n",
    "    'EcoliK12':'Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.cds.all.fa',\\\n",
    "    'EcoliRel606':'Escherichia_coli_b_str_rel606.ASM1798v1.cds.all.fa',\\\n",
    "    'BW25113':'Escherichia_coli_bw25113.ASM75055v1.cds.all.fa',\\\n",
    "    'Leprae':'Mycobacterium_leprae_br4923.ASM2668v1.cds.all.fa',\\\n",
    "    'Tuberc':'Mycobacterium_tuberculosis_h37rv.ASM19595v2.cds.all.fa',\\\n",
    "    'H37Ra': 'Mycobacterium_tuberculosis_h37ra.ASM1614v1.cds.all.fa',\\\n",
    "    'Haemophilum': 'Mycobacterium_haemophilum_gca_001021415.ASM102141v1.cds.all.fa'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_home='DataPairOfSpecies/'+list_sp[0]+list_sp[1]+'/'\n",
    "\n",
    "#create a directory where I will store info about this two species and thier alignmants\n",
    "os.mkdir(folder_home) \n",
    "\n",
    "#create a directory for PAML results (the algorithm I will launch is called codeml)\n",
    "os.mkdir(folder_home+'codeml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H37Ra\n",
      "Haemophilum\n"
     ]
    }
   ],
   "source": [
    "#parse CDS fasta files ~2 min\n",
    "dict_fasta={}\n",
    "for sp in list_sp:\n",
    "    print(sp)\n",
    "    tmp_list_seq=[];tmp_list_id=[];tmp_list_descr=[];\n",
    "    \n",
    "    #read fasta file\n",
    "    for record in SeqIO.parse(\"Data/cds_fasta/\"+dist_sp[sp], \"fasta\"):\n",
    "        tmp_list_seq.append(str(record.seq))\n",
    "        tmp_list_id.append(record.id)\n",
    "        tmp_list_descr.append(record.description)\n",
    "\n",
    "    #save fasta in a dataframe\n",
    "    dict_fasta[sp]=pd.DataFrame([[tmp_list_id[i],tmp_list_descr[i],tmp_list_seq[i]] for i in range(len(tmp_list_id))],\\\n",
    "            columns=['id','descr','seq'])\n",
    "    \n",
    "    #calculate length of sequence\n",
    "    dict_fasta[sp]['len']=dict_fasta[sp]['seq'].apply(lambda x: len(x))\n",
    "    #keep only those genes which nucleotyde sequence length %3. (this removes only a few genes)\n",
    "    dict_fasta[sp]=dict_fasta[sp][dict_fasta[sp]['len'].apply(lambda x: x%3)==0]\n",
    "    dict_fasta[sp]['len_aa']=dict_fasta[sp]['len'].apply(lambda x: x/3)\n",
    "    \n",
    "    #cut gene id from description field. \n",
    "    dict_fasta[sp]['id_gene']=dict_fasta[sp]['descr'].apply(lambda x: x.split('gene:')[1].split()[0])\n",
    "    dict_fasta[sp]['id_gene']=dict_fasta[sp]['id_gene'].apply(lambda x: x.split('.')[0])\n",
    "    #cut symbolic gene id from description field. \n",
    "    dict_fasta[sp]['id_symbol']=dict_fasta[sp]['descr'].\\\n",
    "    apply(lambda x: x.split('gene_symbol:')[1].split()[0] if 'gene_symbol:' in x else np.nan)\n",
    "    \n",
    "    #translate nuleotyde sequence into amino acid sequence \n",
    "    dict_fasta[sp]['seq_aa']=dict_fasta[sp]['seq'].apply(lambda x: str(Seq(x).translate()))\n",
    "\n",
    "    #important step! Original file contains all isoforms for a gene. \n",
    "    #Here I sort by length and keep the longest isoform. \n",
    "    #(It is not ideal, maybe we can find somewhere information about \"main\" isoform)\n",
    "    dict_fasta[sp]=dict_fasta[sp].sort_values(by='len',ascending=False).drop_duplicates('id_gene')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save CDS fasta for aa and nuc  \n",
    "for sp in list_sp:\n",
    "    text=''\n",
    "    for id_transcript, seq_aa in dict_fasta[sp][['id','seq_aa']].values:\n",
    "        text+='>'+id_transcript+'\\n'+seq_aa[:-1]+'\\n'\n",
    "\n",
    "    fout=open(folder_home+'cds_fasta_aa_'+sp+'.fasta','w')\n",
    "    fout.write(text)\n",
    "    fout.close()\n",
    "    \n",
    "for sp in list_sp:\n",
    "    text=''\n",
    "    for id_transcript, seq_aa in dict_fasta[sp][['id','seq']].values:\n",
    "        text+='>'+id_transcript+'\\n'+seq_aa[:-1]+'\\n'\n",
    "\n",
    "    fout=open(folder_home+'cds_fasta_nuc_'+sp+'.fasta','w')\n",
    "    fout.write(text)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLAST with Diamond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diamond is analogous software to BLAST, but works 10 times faster.\n",
    "#I launch it from python. \"!\" in jupyter notebook makes the line to be execcuted as it was typed in the command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "diamond v2.0.2.140 (C) Max Planck Society for the Advancement of Science\n",
      "Documentation, support and updates available at http://www.diamondsearch.org\n",
      "\n",
      "#CPU threads: 8\n",
      "Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)\n",
      "Database input file: cds_fasta_aa_H37Ra.fasta\n",
      "Opening the database file...  [0.002s]\n",
      "Loading sequences...  [0.027s]\n",
      "Masking sequences...  [0.048s]\n",
      "Writing sequences...  [0s]\n",
      "Hashing sequences...  [0s]\n",
      "Loading sequences...  [0s]\n",
      "Writing trailer...  [0s]\n",
      "Closing the input file...  [0.009s]\n",
      "Closing the database file...  [0.001s]\n",
      "Database hash = 20cd80090af06d2c66f20eaf10ec46bd\n",
      "Processed 4034 sequences, 1340588 letters.\n",
      "Total time = 0.09s\n",
      "diamond v2.0.2.140 (C) Max Planck Society for the Advancement of Science\n",
      "Documentation, support and updates available at http://www.diamondsearch.org\n",
      "\n",
      "#CPU threads: 8\n",
      "Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)\n",
      "Database input file: cds_fasta_aa_Haemophilum.fasta\n",
      "Opening the database file...  [0s]\n",
      "Loading sequences...  [0.017s]\n",
      "Masking sequences...  [0.038s]\n",
      "Writing sequences...  [0s]\n",
      "Hashing sequences...  [0s]\n",
      "Loading sequences...  [0s]\n",
      "Writing trailer...  [0s]\n",
      "Closing the input file...  [0.006s]\n",
      "Closing the database file...  [0s]\n",
      "Database hash = be871c4c355f6cce521977bae2e43e58\n",
      "Processed 3730 sequences, 1202496 letters.\n",
      "Total time = 0.067s\n",
      "diamond v2.0.2.140 (C) Max Planck Society for the Advancement of Science\n",
      "Documentation, support and updates available at http://www.diamondsearch.org\n",
      "\n",
      "#CPU threads: 8\n",
      "Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)\n",
      "Temporary directory: \n",
      "Opening the database...  [0.038s]\n",
      "#Target sequences to report alignments for: 25\n",
      "Reference = db_Haemophilum.dmnd\n",
      "Sequences = 3730\n",
      "Letters = 1202496\n",
      "Block size = 2000000000\n",
      "Opening the input file...  [0.02s]\n",
      "Opening the output file...  [0s]\n",
      "Loading query sequences...  [0.013s]\n",
      "Masking queries...  [0.049s]\n",
      "Building query seed set...  [0.011s]\n",
      "Algorithm: Double-indexed\n",
      "Building query histograms...  [0.01s]\n",
      "Allocating buffers...  [0s]\n",
      "Loading reference sequences...  [0.001s]\n",
      "Masking reference...  [0.039s]\n",
      "Initializing temporary storage...  [0.002s]\n",
      "Building reference histograms...  [0.015s]\n",
      "Allocating buffers...  [0s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 0.\n",
      "Building reference seed array...  [0.012s]\n",
      "Building query seed array...  [0.009s]\n",
      "Computing hash join...  [0.004s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.02s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 1.\n",
      "Building reference seed array...  [0.008s]\n",
      "Building query seed array...  [0.011s]\n",
      "Computing hash join...  [0.004s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.02s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 2.\n",
      "Building reference seed array...  [0.014s]\n",
      "Building query seed array...  [0.009s]\n",
      "Computing hash join...  [0.004s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.017s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 3.\n",
      "Building reference seed array...  [0.012s]\n",
      "Building query seed array...  [0.009s]\n",
      "Computing hash join...  [0.004s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.016s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 0.\n",
      "Building reference seed array...  [0.013s]\n",
      "Building query seed array...  [0.012s]\n",
      "Computing hash join...  [0.003s]\n",
      "Building seed filter...  [0.001s]\n",
      "Searching alignments...  [0.015s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 1.\n",
      "Building reference seed array...  [0.011s]\n",
      "Building query seed array...  [0.01s]\n",
      "Computing hash join...  [0.003s]\n",
      "Building seed filter...  [0.001s]\n",
      "Searching alignments...  [0.016s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 2.\n",
      "Building reference seed array...  [0.015s]\n",
      "Building query seed array...  [0.014s]\n",
      "Computing hash join...  [0.003s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.014s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 3.\n",
      "Building reference seed array...  [0.012s]\n",
      "Building query seed array...  [0.012s]\n",
      "Computing hash join...  [0.005s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.014s]\n",
      "Deallocating buffers...  [0s]\n",
      "Clearing query masking...  [0s]\n",
      "Computing alignments...  [1.637s]\n",
      "Deallocating reference...  [0s]\n",
      "Loading reference sequences...  [0s]\n",
      "Deallocating buffers...  [0s]\n",
      "Deallocating queries...  [0s]\n",
      "Loading query sequences...  [0s]\n",
      "Closing the input file...  [0.006s]\n",
      "Closing the output file...  [0.003s]\n",
      "Closing the database file...  [0.006s]\n",
      "Deallocating taxonomy...  [0s]\n",
      "Total time = 2.25s\n",
      "Reported 8689 pairwise alignments, 8689 HSPs.\n",
      "3185 queries aligned.\n",
      "diamond v2.0.2.140 (C) Max Planck Society for the Advancement of Science\n",
      "Documentation, support and updates available at http://www.diamondsearch.org\n",
      "\n",
      "#CPU threads: 8\n",
      "Scoring parameters: (Matrix=BLOSUM62 Lambda=0.267 K=0.041 Penalties=11/1)\n",
      "Temporary directory: \n",
      "Opening the database...  [0.026s]\n",
      "#Target sequences to report alignments for: 25\n",
      "Reference = db_H37Ra.dmnd\n",
      "Sequences = 4034\n",
      "Letters = 1340588\n",
      "Block size = 2000000000\n",
      "Opening the input file...  [0.01s]\n",
      "Opening the output file...  [0s]\n",
      "Loading query sequences...  [0.008s]\n",
      "Masking queries...  [0.049s]\n",
      "Building query seed set...  [0.014s]\n",
      "Algorithm: Double-indexed\n",
      "Building query histograms...  [0.015s]\n",
      "Allocating buffers...  [0s]\n",
      "Loading reference sequences...  [0.003s]\n",
      "Masking reference...  [0.047s]\n",
      "Initializing temporary storage...  [0.004s]\n",
      "Building reference histograms...  [0.012s]\n",
      "Allocating buffers...  [0s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 0.\n",
      "Building reference seed array...  [0.015s]\n",
      "Building query seed array...  [0.013s]\n",
      "Computing hash join...  [0.005s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.017s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 1.\n",
      "Building reference seed array...  [0.013s]\n",
      "Building query seed array...  [0.01s]\n",
      "Computing hash join...  [0.004s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.017s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 2.\n",
      "Building reference seed array...  [0.011s]\n",
      "Building query seed array...  [0.013s]\n",
      "Computing hash join...  [0.003s]\n",
      "Building seed filter...  [0.001s]\n",
      "Searching alignments...  [0.013s]\n",
      "Processing query block 0, reference block 0, shape 0, index chunk 3.\n",
      "Building reference seed array...  [0.012s]\n",
      "Building query seed array...  [0.009s]\n",
      "Computing hash join...  [0.004s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.013s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 0.\n",
      "Building reference seed array...  [0.013s]\n",
      "Building query seed array...  [0.009s]\n",
      "Computing hash join...  [0.004s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.015s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 1.\n",
      "Building reference seed array...  [0.015s]\n",
      "Building query seed array...  [0.008s]\n",
      "Computing hash join...  [0.004s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.016s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 2.\n",
      "Building reference seed array...  [0.016s]\n",
      "Building query seed array...  [0.011s]\n",
      "Computing hash join...  [0.004s]\n",
      "Building seed filter...  [0.003s]\n",
      "Searching alignments...  [0.013s]\n",
      "Processing query block 0, reference block 0, shape 1, index chunk 3.\n",
      "Building reference seed array...  [0.011s]\n",
      "Building query seed array...  [0.009s]\n",
      "Computing hash join...  [0.004s]\n",
      "Building seed filter...  [0.002s]\n",
      "Searching alignments...  [0.016s]\n",
      "Deallocating buffers...  [0s]\n",
      "Clearing query masking...  [0s]\n",
      "Computing alignments...  [1.612s]\n",
      "Deallocating reference...  [0s]\n",
      "Loading reference sequences...  [0s]\n",
      "Deallocating buffers...  [0s]\n",
      "Deallocating queries...  [0s]\n",
      "Loading query sequences...  [0s]\n",
      "Closing the input file...  [0.005s]\n",
      "Closing the output file...  [0.002s]\n",
      "Closing the database file...  [0.006s]\n",
      "Deallocating taxonomy...  [0s]\n",
      "Total time = 2.203s\n",
      "Reported 8068 pairwise alignments, 8068 HSPs.\n",
      "3045 queries aligned.\n"
     ]
    }
   ],
   "source": [
    "#pairwise blast with diamond. ~10 min\n",
    "os.chdir(folder_home)\n",
    "\n",
    "for sp in list_sp:\n",
    "    !diamond makedb --in cds_fasta_aa_{sp}.fasta --db db_{sp}\n",
    "    \n",
    "out_format='qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qseq_gapped sseq_gapped'\n",
    "for sp1,sp2 in [list_sp,list_sp[::-1]]:\n",
    "    !diamond blastp --query cds_fasta_aa_{sp1}.fasta --db db_{sp2}.dmnd --tantan-minMaskProb 1.0 --outfmt 6 {out_format} --out blast_{sp1}.vs.{sp2}.txt\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse blast results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2824\n",
      "2862\n",
      "Haemophilum 2824 2862 3119 2601\n"
     ]
    }
   ],
   "source": [
    "#parse blast results\n",
    "\n",
    "sp1_0=list_sp[0] #query\n",
    "sp2_0=list_sp[1] #target\n",
    "blast_dir={}\n",
    "\n",
    "#read blast results for forward (Human->Mouse) and backward (Mouse->Human) directions\n",
    "for sp1,sp2,direct in [[sp1_0,sp2_0,'fwd'],[sp2_0,sp1_0,'back']]:\n",
    "    \n",
    "    #read all blast results\n",
    "    df=pd.read_csv(folder_home+'blast_'+sp1+'.vs.'+sp2+'.txt',sep='\\t',header=None)\n",
    "    #rename columns\n",
    "    df=df.rename(columns={0:'id_'+sp1,1:'id_'+sp2,2:'ident',3:'len',4:'mism',5:'gaps',\\\n",
    "                      6:'qlo',7:'qhi',8:'tlo',9:'thi',10:'evalue',11:'bits',12:'qseq',13:'sseq'})\n",
    "    \n",
    "    #merge blast results with other information stored in the dict_fasta\n",
    "    df=df.merge(dict_fasta[sp1][['id','id_gene','len_aa']].\\\n",
    "                rename(columns={'id':'id_'+sp1,'id_gene':'id_gene_'+sp1,'len_aa':'qlen'}),on='id_'+sp1,how='left')\n",
    "    df=df.merge(dict_fasta[sp2][['id','id_gene','len_aa']].\\\n",
    "                rename(columns={'id':'id_'+sp2,'id_gene':'id_gene_'+sp2,'len_aa':'tlen'}),on='id_'+sp2,how='left')\n",
    "    \n",
    "    #keep only those results, where alignment length>30 and it covers more than 80% of protein length, \n",
    "    #and e-value is quite significant\n",
    "    df=df[(df['len']>30)&\\\n",
    "          (df['qhi']-df['qlo']>0.8*df['qlen'])&(df['thi']-df['tlo']>0.8*df['tlen'])\\\n",
    "         &(df['evalue']<10**-6)]\n",
    "    \n",
    "    #keep only best hit for each gene.\n",
    "    # it is a bit complicated, beacause I wanted to keep several hits it they are the same good.\n",
    "    \n",
    "    #first, I group lines by gene id and save the maximum identity for each gene in a df_max\n",
    "    name_id='id_gene_'+sp1\n",
    "    df_max=df[[name_id,'ident']].groupby(name_id).max() \n",
    "    df_max[name_id]=df_max.index\n",
    "    df_max=df_max.set_index(df_max[name_id].values)\n",
    "    df_max=df_max.rename(columns={'ident':'ident_max'}) \n",
    "    #second, I merge column \"ident_max\" to the main dataframe df\n",
    "    df=df.merge(df_max, on=name_id,how='left')\n",
    "    #third, I keep only those entries where ident==ident_max\n",
    "    df=df[(df['ident']==df['ident_max'])]\n",
    "  \n",
    "    blast_dir[direct]=df\n",
    "    print(len(df))\n",
    "    \n",
    "#merge forward and backward results of blast\n",
    "blast=blast_dir['fwd'].merge(blast_dir['back'],on='id_'+sp2_0,how='inner',suffixes=['_'+sp1_0,'_'+sp2_0])\n",
    "\n",
    "#keep only reciprocal hits\n",
    "blast=blast[blast['id_gene_'+sp1_0+'_'+sp1_0]==blast['id_gene_'+sp1_0+'_'+sp2_0]]\n",
    "blast=blast.drop_duplicates('id_gene_'+sp1_0+'_'+sp1_0)\n",
    "\n",
    "print(sp2_0,len(blast_dir['fwd']),len(blast_dir['back']),len(blast_dir['fwd'].merge(blast_dir['back'],on='id_'+sp2_0,how='inner')),len(blast))\n",
    "\n",
    "blast['id']=blast['id_gene_'+list_sp[0]+'_'+list_sp[0]]\n",
    "blast=blast.set_index(blast['id'].values)\n",
    "\n",
    "sp1=list_sp[0]\n",
    "sp2=list_sp[1]\n",
    "blast['id_gene_'+sp1]=blast['id_gene_'+sp1+'_'+sp1]\n",
    "blast['id_gene_'+sp2]=blast['id_gene_'+sp2+'_'+sp1]\n",
    "blast=blast.rename(columns={'id_'+sp1+'_'+sp1:'id_'+sp1})\n",
    "#merge  blast results and sequences\n",
    "for sp in list_sp:\n",
    "    blast=blast.merge(dict_fasta[sp].rename(columns={'id':'id_'+sp,'seq_aa':'seq_aa_'+sp,'seq':'seq_'+sp})\\\n",
    "    [['id_'+sp,'seq_aa_'+sp,'seq_'+sp]],on='id_'+sp,how='left')\n",
    "blast=blast.set_index(blast['id_gene_'+list_sp[0]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make nucl alignments by blast alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make nucl alignments by blast alignments. takes ~2 min   \n",
    "sp1=list_sp[0]\n",
    "sp2=list_sp[1]\n",
    "for i in range(len(blast)):\n",
    "    id_blast=blast.index[i]\n",
    "    id1=blast.iloc[i]['id_gene_'+sp1]\n",
    "    id2=blast.iloc[i]['id_gene_'+sp2]\n",
    "    #whole amino acid sequence\n",
    "    seq_aa1=blast.iloc[i]['seq_aa_'+sp1] \n",
    "    seq_aa2=blast.iloc[i]['seq_aa_'+sp2]\n",
    "    #whole nucleotyde sequence\n",
    "    seq_nuc1=blast.iloc[i]['seq_'+sp1] \n",
    "    seq_nuc2=blast.iloc[i]['seq_'+sp2]\n",
    "    #sequnce of align part. With gaps inserted \n",
    "    seq_aa1_align=blast.iloc[i]['qseq_'+sp1] \n",
    "    seq_aa2_align=blast.iloc[i]['sseq_'+sp1]\n",
    "    #srart and end of alignment\n",
    "    start1=blast.iloc[i]['qlo_'+sp1]\n",
    "    start2=blast.iloc[i]['tlo_'+sp1]\n",
    "    end1=blast.iloc[i]['qhi_'+sp1]\n",
    "    end2=blast.iloc[i]['thi_'+sp1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    for sp,seq_aa,seq_nuc in [[sp1,seq_aa1_align, seq_nuc1[3*(start1-1):3*end1] ],\\\n",
    "                              [sp2,seq_aa2_align, seq_nuc2[3*(start2-1):3*end2] ]]:\n",
    "        dna=''\n",
    "        #I go through alignmnet and substitute each amino acid with corresponding nucleotyde codon or with '---' if it is gap \n",
    "        for aa in seq_aa:\n",
    "            if aa=='-':\n",
    "                dna+='---'\n",
    "            elif aa=='*': \n",
    "                # \"*\" denotes stop-codons.\n",
    "                dna+='---'\n",
    "                seq_nuc=seq_nuc[3:]\n",
    "                if len(seq_nuc)>0:\n",
    "                    # this means that stop codon was found in the middle of the protein, such errors sometimes appear\n",
    "                    # I print proteins where it happens. but so far ignore it.\n",
    "                    print(i)\n",
    "            else:\n",
    "                dna+=seq_nuc[:3]\n",
    "                seq_nuc=seq_nuc[3:]\n",
    "               \n",
    "        blast.at[id_blast,'seq_nuc_align_'+sp]=dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make text file with all alignments\n",
    "\n",
    "sp1=list_sp[0]\n",
    "sp2=list_sp[1]\n",
    "text=''\n",
    "\n",
    "for i in range(len(blast)):\n",
    "    id1=blast.iloc[i]['id_gene_'+sp1]\n",
    "    id2=blast.iloc[i]['id_gene_'+sp2]\n",
    "    seq_nuc1=blast.iloc[i]['seq_nuc_align_'+sp1] \n",
    "    seq_nuc2=blast.iloc[i]['seq_nuc_align_'+sp2]\n",
    "    \n",
    "    \n",
    "    text+='2\\t'+str(len(seq_nuc1))+'\\n>'+id1+'\\n'+seq_nuc1+'\\n'+'>'+id2+'\\n'+seq_nuc2+'\\n\\n'\n",
    "\n",
    "# write text file with all pairs of sequencies - the input for codeml\n",
    "fout=open(folder_home+'codeml/all_pairs.fasta','w')\n",
    "fout.write(text)\n",
    "fout.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAML to calculate dN and dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout=open(folder_home+'species.tree','w')\n",
    "fout.write('(%s,%s);' %(list_sp[0],list_sp[1]))\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codeml from PAML to calculate dN, dS. ~10 min\n",
    "# I launch PAML via biopython. I don't remember either PAML is fully installed if install biopython, or\n",
    "# one need to install PAML modules separately. \n",
    "# \"species.tree\" file contains species tree, which is simple \"(sp1,sp2);\" in our case\n",
    "\n",
    "# Because I constantly need to rerun. DELETE FOR DINARA\n",
    "os.chdir(\"C:\\\\Users\\\\sysem\\\\Documents\\\\EvolutionRates\\\\Get_dN_dS\")\n",
    "# DELETE FOR DINARA\n",
    "\n",
    "\n",
    "cml = codeml.Codeml(alignment = folder_home+'codeml/all_pairs.fasta',\\\n",
    "                    tree=folder_home+'species.tree',\\\n",
    "                    out_file = folder_home+'codeml/results.out',\\\n",
    "                    working_dir = folder_home+\"codeml/\")\n",
    "\n",
    "# just keep these parameters as they are here\n",
    "cml.set_options(\n",
    "                noisy='9',\\\n",
    "                verbose='2',\\\n",
    "                runmode='-2',\\\n",
    "                seqtype='1',\\\n",
    "                model='0', \\\n",
    "                cleandata = '1',\\\n",
    "               CodonFreq = '2',\\\n",
    "               NSsites = '0',\\\n",
    "               fix_omega = '0',\\\n",
    "                ndata=len(blast)\n",
    ")\n",
    "# DELETE FOR DINARA TOO\n",
    "res=cml.run(command=\"C:\\\\Users\\\\sysem\\\\Apps\\\\PAML\\\\paml4.9j\\\\bin\\\\codeml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect CODEML pairwise results\n",
    "tmp=[]\n",
    "with open(folder_home+'codeml/rst') as fin:\n",
    "    for line in fin.readlines()[1+4::6]:\n",
    "        tmp.append(map(lambda x: float(x),line.split()[2:7]))\n",
    "df_pairs=pd.DataFrame(tmp,columns=['N', 'S', 'dN', 'dS', 'dN/dS'])\n",
    "\n",
    "for col in ['N', 'S', 'dN', 'dS', 'dN/dS']:\n",
    "    blast[col]=df_pairs[col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "blast.to_csv(folder_home+'blast_clock_all.txt',sep='\\t',index=True)\n",
    "\n",
    "sp1=list_sp[0]\n",
    "sp2=list_sp[1]\n",
    "\n",
    "blast.rename(columns={'id_gene_'+sp1+'_'+sp1:'id_'+sp1,'id_gene_'+sp2+'_'+sp1:'id_'+sp2,\\\n",
    "                      'id_'+sp2:'id_isoform_'+sp2,'id_'+sp1:'id_isoform_'+sp1,\\\n",
    "                     'ident_'+sp1:'ident','len_'+sp1:'len','mism_'+sp1:'mism','gaps_'+sp1:'gaps',\\\n",
    "                     'qlo_'+sp1:'qlo','qhi_'+sp1:'qhi','tlo_'+sp1:'tlo','thi_'+sp1:'thi',\\\n",
    "                     'evalue_'+sp1:'evalue','bits_'+sp1:'bits',\\\n",
    "                     'qlen_'+sp1:'qlen','tlen_'+sp1:'tlen'})\\\n",
    "[['id_'+sp1,'id_'+sp2,'dN','dS','dN/dS','N','S','id_isoform_'+sp1,'id_isoform_'+sp2,'ident','len','qlen','tlen','mism','gaps',\\\n",
    " 'qlo','qhi','tlo','thi','evalue','bits','seq_aa_'+sp1,'seq_aa_'+sp2,'ident_max_'+sp2]]\\\n",
    ".to_csv(folder_home+'blast_clock_short_seq.txt',sep='\\t',index=True)\n",
    "\n",
    "blast.rename(columns={'id_gene_'+sp1+'_'+sp1:'id_'+sp1,'id_gene_'+sp2+'_'+sp1:'id_'+sp2,\\\n",
    "                      'id_'+sp2:'id_isoform_'+sp2,'id_'+sp1:'id_isoform_'+sp1,\\\n",
    "                     'ident_'+sp1:'ident','len_'+sp1:'len','mism_'+sp1:'mism','gaps_'+sp1:'gaps',\\\n",
    "                     'qlo_'+sp1:'qlo','qhi_'+sp1:'qhi','tlo_'+sp1:'tlo','thi_'+sp1:'thi',\\\n",
    "                     'evalue_'+sp1:'evalue','bits_'+sp1:'bits',\\\n",
    "                     'qlen_'+sp1:'qlen','tlen_'+sp1:'tlen'})\\\n",
    "[['id_'+sp1,'id_'+sp2,'dN','dS','dN/dS','N','S','id_isoform_'+sp1,'id_isoform_'+sp2,'ident','len','qlen','tlen','mism','gaps',\\\n",
    " 'qlo','qhi','tlo','thi','evalue','bits','ident_max_'+sp2]]\\\n",
    ".to_csv(folder_home+'blast_clock_short.txt',sep='\\t',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save=blast[['id','dN','dS']]\n",
    "to_save=to_save.merge(dict_fasta[list_sp[0]][['id_gene','id_symbol']].rename(columns={'id_gene':'id'}),on='id',how='left')\n",
    "to_save.loc[pd.isnull(to_save['id_symbol']),'id_symbol']=to_save.loc[pd.isnull(to_save['id_symbol']),'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save.to_csv(folder_home+'EvRate.txt',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
